{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Solar Energy Prediction Using Machine Learning\n",
        "\n",
        "This notebook implements a machine learning model to predict solar energy output based on weather and environmental features.\n",
        "\n",
        "## Objectives:\n",
        "1. Load and explore raw solar energy data\n",
        "2. Preprocess and clean the dataset\n",
        "3. Split data into training and testing sets\n",
        "4. Train multiple machine learning models\n",
        "5. Evaluate and compare model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('Libraries imported successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Raw Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the raw dataset\n",
        "df_raw = pd.read_csv('raw_solar_data.csv')\n",
        "\n",
        "print(f'Dataset Shape: {df_raw.shape}')\n",
        "print(f'\\nFirst 5 records:')\n",
        "print(df_raw.head())\n",
        "\n",
        "print(f'\\nDataset Information:')\n",
        "print(df_raw.info())\n",
        "\n",
        "print(f'\\nBasic Statistics:')\n",
        "print(df_raw.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Exploration and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print('Missing Values:')\n",
        "print(df_raw.isnull().sum())\n",
        "\n",
        "# Visualize distribution of Solar Energy Output\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.hist(df_raw['Solar_Energy_Output'], bins=30, edgecolor='black')\n",
        "plt.title('Distribution of Solar Energy Output')\n",
        "plt.xlabel('Solar Energy (kWh)')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(df_raw['Solar_Irradiance'], df_raw['Solar_Energy_Output'], alpha=0.5)\n",
        "plt.title('Solar Irradiance vs Energy Output')\n",
        "plt.xlabel('Solar Irradiance (W/m\u00b2)')\n",
        "plt.ylabel('Energy Output (kWh)')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.scatter(df_raw['Cloud_Cover'], df_raw['Solar_Energy_Output'], alpha=0.5, color='orange')\n",
        "plt.title('Cloud Cover vs Energy Output')\n",
        "plt.xlabel('Cloud Cover (%)')\n",
        "plt.ylabel('Energy Output (kWh)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Data exploration complete!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for preprocessing\n",
        "df_processed = df_raw.copy()\n",
        "\n",
        "# Step 1: Remove duplicates\n",
        "print(f'Records before removing duplicates: {len(df_processed)}')\n",
        "df_processed = df_processed.drop_duplicates().reset_index(drop=True)\n",
        "print(f'Records after removing duplicates: {len(df_processed)}')\n",
        "\n",
        "# Step 2: Remove outliers using IQR method\n",
        "def remove_outliers(dataframe, columns):\n",
        "    df_temp = dataframe.copy()\n",
        "    for col in columns:\n",
        "        Q1 = df_temp[col].quantile(0.25)\n",
        "        Q3 = df_temp[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        df_temp = df_temp[(df_temp[col] >= lower_bound) & (df_temp[col] <= upper_bound)]\n",
        "    return df_temp\n",
        "\n",
        "numeric_cols = df_processed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "numeric_cols.remove('Hour')\n",
        "numeric_cols.remove('Month')\n",
        "\n",
        "print(f'\\nRecords before removing outliers: {len(df_processed)}')\n",
        "df_processed = remove_outliers(df_processed, numeric_cols)\n",
        "print(f'Records after removing outliers: {len(df_processed)}')\n",
        "\n",
        "print(f'\\nDataset shape after cleaning: {df_processed.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create new features\n",
        "df_processed['Is_Daylight'] = ((df_processed['Hour'] >= 6) & (df_processed['Hour'] <= 18)).astype(int)\n",
        "df_processed['Season'] = df_processed['Month'].apply(lambda x: \n",
        "    'Winter' if x in [12, 1, 2] else \n",
        "    'Spring' if x in [3, 4, 5] else \n",
        "    'Summer' if x in [6, 7, 8] else 'Fall')\n",
        "\n",
        "# Encode categorical features\n",
        "season_mapping = {'Winter': 0, 'Spring': 1, 'Summer': 2, 'Fall': 3}\n",
        "df_processed['Season_Encoded'] = df_processed['Season'].map(season_mapping)\n",
        "\n",
        "print('New features created:')\n",
        "print(f'- Is_Daylight')\n",
        "print(f'- Season')\n",
        "print(f'- Season_Encoded')\n",
        "print(f'\\nDataset shape after feature engineering: {df_processed.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Scaling and Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop unnecessary columns\n",
        "df_model = df_processed.drop(['Date', 'Season'], axis=1)\n",
        "\n",
        "# Separate features and target\n",
        "X = df_model.drop('Solar_Energy_Output', axis=1)\n",
        "y = df_model['Solar_Energy_Output']\n",
        "\n",
        "print(f'Features shape: {X.shape}')\n",
        "print(f'Target shape: {y.shape}')\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
        "\n",
        "print(f'\\nFeatures after scaling:')\n",
        "print(X_scaled.head())\n",
        "print(f'\\nFeature statistics after scaling:')\n",
        "print(X_scaled.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training set size: {len(X_train)}')\n",
        "print(f'Testing set size: {len(X_test)}')\n",
        "print(f'\\nTraining features shape: {X_train.shape}')\n",
        "print(f'Testing features shape: {X_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Train models\n",
        "trained_models = {}\n",
        "for name, model in models.items():\n",
        "    print(f'Training {name}...')\n",
        "    model.fit(X_train, y_train)\n",
        "    trained_models[name] = model\n",
        "    print(f'{name} trained successfully!\\n')\n",
        "\n",
        "print('All models trained successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate models\n",
        "results = {}\n",
        "\n",
        "print('='*70)\n",
        "print('MODEL EVALUATION RESULTS')\n",
        "print('='*70)\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "    \n",
        "    # Metrics\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "    test_r2 = r2_score(y_test, y_test_pred)\n",
        "    \n",
        "    results[name] = {\n",
        "        'train_mse': train_mse,\n",
        "        'test_mse': test_mse,\n",
        "        'train_mae': train_mae,\n",
        "        'test_mae': test_mae,\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2\n",
        "    }\n",
        "    \n",
        "    print(f'\\n{name}:')\n",
        "    print(f'  Train MSE: {train_mse:.4f}, Test MSE: {test_mse:.4f}')\n",
        "    print(f'  Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}')\n",
        "    print(f'  Train R\u00b2: {train_r2:.4f}, Test R\u00b2: {test_r2:.4f}')\n",
        "\n",
        "print('\\n' + '='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualization of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# MSE Comparison\n",
        "model_names = list(results.keys())\n",
        "train_mses = [results[m]['train_mse'] for m in model_names]\n",
        "test_mses = [results[m]['test_mse'] for m in model_names]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "axes[0, 0].bar(x - width/2, train_mses, width, label='Train', alpha=0.8)\n",
        "axes[0, 0].bar(x + width/2, test_mses, width, label='Test', alpha=0.8)\n",
        "axes[0, 0].set_ylabel('MSE')\n",
        "axes[0, 0].set_title('Mean Squared Error Comparison')\n",
        "axes[0, 0].set_xticks(x)\n",
        "axes[0, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# MAE Comparison\n",
        "train_maes = [results[m]['train_mae'] for m in model_names]\n",
        "test_maes = [results[m]['test_mae'] for m in model_names]\n",
        "\n",
        "axes[0, 1].bar(x - width/2, train_maes, width, label='Train', alpha=0.8, color='green')\n",
        "axes[0, 1].bar(x + width/2, test_maes, width, label='Test', alpha=0.8, color='lightgreen')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].set_title('Mean Absolute Error Comparison')\n",
        "axes[0, 1].set_xticks(x)\n",
        "axes[0, 1].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# R\u00b2 Score Comparison\n",
        "train_r2s = [results[m]['train_r2'] for m in model_names]\n",
        "test_r2s = [results[m]['test_r2'] for m in model_names]\n",
        "\n",
        "axes[1, 0].bar(x - width/2, train_r2s, width, label='Train', alpha=0.8, color='orange')\n",
        "axes[1, 0].bar(x + width/2, test_r2s, width, label='Test', alpha=0.8, color='lightsalmon')\n",
        "axes[1, 0].set_ylabel('R\u00b2 Score')\n",
        "axes[1, 0].set_title('R\u00b2 Score Comparison')\n",
        "axes[1, 0].set_xticks(x)\n",
        "axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Best Model Predictions vs Actual\n",
        "best_model_name = max(results, key=lambda x: results[x]['test_r2'])\n",
        "best_model = trained_models[best_model_name]\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\n",
        "axes[1, 1].scatter(y_test, y_test_pred, alpha=0.5)\n",
        "axes[1, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1, 1].set_xlabel('Actual Values')\n",
        "axes[1, 1].set_ylabel('Predicted Values')\n",
        "axes[1, 1].set_title(f'Best Model ({best_model_name}) - Predictions vs Actual')\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Best Model: {best_model_name} (R\u00b2 = {results[best_model_name][\"test_r2\"]:.4f})')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Feature Importance (for Tree-based Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance from Random Forest\n",
        "rf_model = trained_models['Random Forest']\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': rf_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print('Feature Importance (Top 10):')\n",
        "print(feature_importance.head(10))\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 10 Most Important Features')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Conclusions and Recommendations\n",
        "\n",
        "### Key Findings:\n",
        "1. **Data Quality**: The dataset contained 300 records with 15 features. After outlier removal, 252 clean records remained.\n",
        "2. **Model Performance**: Ensemble methods (Random Forest and Gradient Boosting) outperformed Linear Regression.\n",
        "3. **Key Features**: Solar Irradiance and Cloud Cover are the most important predictors of solar energy output.\n",
        "4. **Recommendations**:\n",
        "   - Use Random Forest or Gradient Boosting for production predictions\n",
        "   - Collect more data to improve model generalization\n",
        "   - Consider time-series features for temporal patterns\n",
        "   - Implement real-time monitoring with model updates"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}